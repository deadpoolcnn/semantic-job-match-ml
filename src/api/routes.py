from fastapi import APIRouter, HTTPException, UploadFile, File
from pydantic import BaseModel, Field
from typing import List, Dict, Any
import logging
import json
from datetime import datetime

from src.services.job_loader import load_jobs
from src.models.matcher import get_job_matcher
from src.services.llm_explainer_service import explain_match_loop
from src.services.resume_parser import parse_resume_file

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api", tags=["match"])

# è¾“å…¥æ¨¡å‹ï¼ˆå‰ç«¯ç®€å†ä¼ è¿‡æ¥ï¼‰
class ResumeInput(BaseModel):
    resume_text: str = Field(..., description="ç®€å†æ–‡æœ¬å†…å®¹")
    top_k: int = Field(default=10, ge=1, le=50, description="è¿”å›åŒ¹é…ç»“æœæ•°é‡")  # å¯é€‰å‚æ•°ï¼Œé»˜è®¤è¿”å›å‰10ä¸ªåŒ¹é…ç»“æœ

# è¾“å‡ºæ¨¡å‹ï¼ˆæ¯ä¸ªjobçš„åŒ¹é…ç»“æœï¼‰
class JobMatch(BaseModel):
    job_id: str
    job_title: str
    company: str
    score: float = Field(..., ge=0.0, le=1.0, description="åŒ¹é…åˆ†æ•°")
    semantic_score: float | None = None
    skill_overlap: float | None = None
    rule_bonus: float | None = None
    why_match: List[str] = Field(default_factory=list)
    skill_gaps: List[str] = Field(default_factory=list)

class MatchResponse(BaseModel):
    matches: List[JobMatch]

@router.post("/match_resume", response_model=MatchResponse)
async def match_resume(resume_input: ResumeInput):
    """
    è¯­ä¹‰åŒ¹é…æ¥å£ï¼šè¾“å…¥ç®€å†æ–‡æœ¬ï¼Œè¾“å‡º Top-K å²—ä½ + è§£é‡Š
    """
    matcher = get_job_matcher() # è·å–å…¨å±€å•ä¾‹çš„ JobMatcher å®ä¾‹
    # è¯­ä¹‰top-KåŒ¹é…ï¼Œè¿”å›å²—ä½ä¿¡æ¯å’ŒåŒ¹é…åˆ†æ•°
    # è¿”å›çš„æ˜¯å­—å…¸ç±»å‹æ•°ç»„
    matched_jobs = matcher.semantic_match(resume_input.resume_text, top_k=resume_input.top_k)
    # 1. è°ƒç”¨ LLM ç”ŸæˆåŒ¹é…è§£é‡Šï¼ˆå¯ä»¥å¹¶è¡ŒåŒ–ï¼‰
    explain_jobs = await explain_match_loop(resume_input.resume_text, matched_jobs)
    # 2. è½¬ä¸º API schemaè¾“å‡º
    matches = [
        JobMatch(
            job_id=job.get("job_id", str(idx)),
            job_title=job.get("job_title", ""),
            company=job.get("company", ""),
            score=job.get("score", 0.0),
            semantic_score=job.get("semantic_score"),
            skill_overlap=job.get("skill_overlap"),
            rule_bonus=job.get("rule_bonus"),
            why_match=job.get("why_match", []), 
            skill_gaps=job.get("skill_gaps", [])
        )
        for idx, job in enumerate(explain_jobs)
    ]
    return MatchResponse(matches=matches)

@router.post("match_resume_file", response_model=MatchResponse)
async def match_resume_file(
    file: UploadFile = File(..., description="PDF or DOCX resume file"),
    top_k: int = 3
):
    """
    æ–‡ä»¶ä¸Šä¼ æ¥å£ï¼šè¾“å…¥ç®€å†æ–‡ä»¶ï¼Œè¾“å‡º Top-K å²—ä½ + è§£é‡Š
    """
    request_id = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    logger.info(f"[{request_id}] ğŸ“¥ Received file upload request")
    logger.info(f"[{request_id}] File: {file.filename}, Content-Type: {file.content_type}, top_k: {top_k}")
    
    # 1. ç±»å‹æ ¡éªŒ
    if file.content_type not in [
        "application/pdf",
        "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    ]:
        logger.warning(f"[{request_id}] âŒ Unsupported file type: {file.content_type}")
        raise HTTPException(status_code=400, detail="Unsupported file type. Please upload a PDF or DOCX file.")
    
    # 2. è¯»å–æ–‡ä»¶å†…å®¹ ä¼ ç»™è§£ææœåŠ¡
    file_bytes = await file.read()
    file_size_kb = len(file_bytes) / 1024
    logger.info(f"[{request_id}] ğŸ“„ File size: {file_size_kb:.2f} KB")
    
    try:
        logger.info(f"[{request_id}] ğŸ” Parsing resume file...")
        parsed = parse_resume_file(file_bytes, file.filename)
        
        # è®°å½•è§£æç»“æœè¯¦æƒ…
        logger.info(f"[{request_id}] âœ… Resume parsed successfully")
        logger.info(f"[{request_id}] Extracted text length: {len(parsed.get('text', ''))} characters")
        logger.info(f"[{request_id}] Extracted skills: {parsed.get('skills', [])}")
        logger.info(f"[{request_id}] Raw data keys: {list(parsed.get('raw', {}).keys())}")
        
        # æ‰“å°å‰200ä¸ªå­—ç¬¦çš„æ–‡æœ¬å†…å®¹
        text_preview = parsed.get('text', '')[:200]
        logger.debug(f"[{request_id}] Text preview: {text_preview}...")
        
    except Exception as e:
        logger.error(f"[{request_id}] âŒ Error parsing resume file: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error parsing resume file: {e}")
    
    resume_text = parsed.get("text", "")
    if not resume_text:
        logger.warning(f"[{request_id}] âš ï¸  No text extracted from resume")
        raise HTTPException(status_code=400, detail="Failed to extract text from resume file.")
    
    # è§£æå‡ºæ¥çš„æŠ€èƒ½,åé¢æ¥è¿› hybrid score
    resume_skills = parsed.get("skills", [])
    logger.info(f"[{request_id}] ğŸ¯ Resume skills for matching: {resume_skills}")
    logger.info(f"[{request_id}] ğŸ¯ Resume skills for matching: {resume_skills}")
    resume_skills_set = set(s.lower() for s in resume_skills) if resume_skills else set()
    
    # 3. è°ƒç”¨ä¹‹å‰çš„æ–‡æœ¬æ¥å£é€»è¾‘
    logger.info(f"[{request_id}] ğŸ” Starting semantic matching...")
    matcher = get_job_matcher() # è·å–å…¨å±€å•ä¾‹çš„ JobMatcher å®ä¾‹
    matched_jobs = matcher.semantic_match(resume_text, top_k=top_k, resume_skills=resume_skills_set)
    
    logger.info(f"[{request_id}] âœ… Found {len(matched_jobs)} matched jobs")
    for i, job in enumerate(matched_jobs[:3], 1):  # åªè®°å½•å‰3ä¸ª
        logger.info(
            f"[{request_id}] Match #{i}: {job.get('job_title')} @ {job.get('company')} | "
            f"score={job.get('score', 0):.3f}, semantic={job.get('semantic_score', 0):.3f}, "
            f"skill_overlap={job.get('skill_overlap', 0):.3f}"
        )
    
    # 4. è°ƒç”¨ LLM ç”ŸæˆåŒ¹é…è§£é‡Šï¼ˆå¯ä»¥å¹¶è¡ŒåŒ–ï¼‰
    logger.info(f"[{request_id}] ğŸ¤– Generating AI explanations...")
    explain_jobs = await explain_match_loop(resume_text, matched_jobs)
    logger.info(f"[{request_id}] âœ… AI explanations generated")
    
    # 5. è½¬ä¸º API schemaè¾“å‡º
    matches = [
        JobMatch(
            job_id=job.get("job_id", str(idx)),
            job_title=job.get("job_title", ""), 
            company=job.get("company", ""),
            score=job.get("score", 0.0),
            semantic_score=job.get("semantic_score"),
            skill_overlap=job.get("skill_overlap"),
            rule_bonus=job.get("rule_bonus"),
            why_match=job.get("why_match", []), 
            skill_gaps=job.get("skill_gaps", [])
        )
        for idx, job in enumerate(explain_jobs)
    ]
    
    logger.info(f"[{request_id}] âœ… Request completed successfully, returning {len(matches)} matches")
    logger.info(f"[{request_id}] {'='*60}")
    
    return MatchResponse(matches=matches)